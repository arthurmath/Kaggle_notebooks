{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd330298",
   "metadata": {},
   "source": [
    "# 👩‍🚀 Deep Understanding of SVD with Visuals\n",
    "\n",
    "What do Image Compression, Linear Regression, and PCA have in common? \n",
    "\n",
    "They all rely on one powerful mathematical tool: the **Singular Value Decomposition (SVD)**.\n",
    "\n",
    "In this notebook, we’ll build an intuitive understanding of SVD using clear visuals, hands-on code, and practical applications. Whether you're compressing images, solving least squares problems, or reducing dimensionality with PCA, SVD is silently working under the engine. We'll explore its theoretical foundations, uncover its geometric meaning, and see it in action across multiple domains.\n",
    "\n",
    "🎯 Goal: demystify SVD by breaking it down step by step and see its applications.\n",
    "\n",
    "\n",
    "### Summary:\n",
    "- 1. Definition of Singuar Values Decomposition\n",
    "   - 1.1 Theorem\n",
    "   - 1.2 Geometric Interpretation\n",
    "   - 1.3 Algorithm\n",
    "- 2. Linear Regression\n",
    "    - 2.1 Theory\n",
    "    - 2.2 Example with two parameters\n",
    "    - 2.3 Time complexity\n",
    "- 3. SVD applied to compression\n",
    "   - 3.1 Reconstruction error\n",
    "   - 3.2 Image compression\n",
    "   - 3.3 Function approximation\n",
    "- 4. PCA\n",
    "   - 4.1 Algorithm\n",
    "   - 4.2 Visual interpretation\n",
    "   - 4.3 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "e9392ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e289b9",
   "metadata": {},
   "source": [
    "## 1. Definition of Singuar Values Decomposition\n",
    "\n",
    "### 1.1 Theorem\n",
    "\n",
    "**Singular Value Decomposition Theorem:** Let $A$ be a real (or complex) $m \\times n$ matrix. The Singular Value Decomposition (SVD) of $A$ is a factorization of the form:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\qquad \\text{with} \\qquad {\\mathbf{\\Sigma}} = \\begin{bmatrix}\n",
    "    \\sigma_1 & 0 & \\cdots & 0   \\\\\n",
    "    0 & \\sigma_2 & \\ddots & \\vdots \\\\\n",
    "    \\vdots & \\ddots & \\ddots & 0 \\\\\n",
    "    \\vdots & & \\ddots &  \\sigma_m\\\\\n",
    "    \\vdots &  &  &  0 \\\\\n",
    "    0 & \\cdots & \\cdots & 0\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix, i.e., $U^T U = I_m$. Its columns are the left singular vectors of $A$.\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix, i.e., $V^T V = I_n$. Its columns are the right singular vectors of $A$.\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with the *singular values* $\\sigma_i$ on the diagonal.\n",
    "\n",
    "The matrix $\\Sigma$ has been represented in the case $n > m$. The $\\sigma_i$ are called the singular values of $\\mathbf{A}$. They are the square roots of the eigenvalues of the matrix ${\\mathbf{A}}^T{\\mathbf{A}}$. They are necessarily *real, positive, or zero*; they can be arranged in any order, but for a given order, the decomposition is unique. From now on, we choose descending order: $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r > 0$ (where $r = \\operatorname{rank}(A)$)\n",
    "\n",
    "The result can also be written as:  \n",
    "$${\\mathbf{A}} = {\\mathbf{U}}{\\mathbf{\\Sigma}}{\\mathbf{V}}^T = \\sum_{i=1}^r \\sigma_i {\\mathbf{u}}_i {\\mathbf{v}}_i^T$$  \n",
    "which expresses the matrix as the sum of $\\operatorname{rank}({\\mathbf{A}})$ matrices of rank-1 contributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e7509",
   "metadata": {},
   "source": [
    "### 1.2 Geometric Interpretation\n",
    "\n",
    "The SVD provides a geometric interpretation of the action of matrix on a set of points. Matris $A$ maps the unit sphere in $\\mathbb{R}^n$ to an ellipsoid in $\\mathbb{R}^m$. The SVD allows us to compute the axis of the ellipsoid, which are oriented along the left singular vectors and are scaled by the singular values of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edbdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate unit circle in R^2\n",
    "theta = np.linspace(0, 2 * np.pi, 200)\n",
    "circle = np.stack([np.cos(theta), np.sin(theta)])  # shape (2, 200)\n",
    "\n",
    "# Step 2: Define a 2x2 matrix A\n",
    "A = np.array([[3, 1], [1, 2]])\n",
    "\n",
    "# Step 3: Compute SVD of A\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "\n",
    "# Step 4: Apply transformation A to the unit circle\n",
    "ellipse = A @ circle\n",
    "\n",
    "# Step 5: Axes of the ellipse are columns of U scaled by singular values\n",
    "ellipse_axes = [S[i] * U[:, i] for i in range(2)]\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(circle[0], circle[1], label=\"Original Circle\", linestyle=\"--\")\n",
    "plt.plot(ellipse[0], ellipse[1], label=\"Transformed Circle\", linewidth=2)\n",
    "\n",
    "# Plot singular vector directions\n",
    "origin = np.zeros(2)\n",
    "for i, vec in enumerate(ellipse_axes):\n",
    "    plt.quiver(*origin, *vec, angles='xy', scale_units='xy', scale=1, color=f\"C{i+2}\", width=0.007, label=f\"$\\sigma_{i+1} u_{i+1}$\")\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title(\"Unit Circle Mapped to Ellipse via A\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58371a5",
   "metadata": {},
   "source": [
    "### 1.3 Algorithm\n",
    "\n",
    "We will now code the SVD in the case of a real square matrix. We therefore write ${\\mathbf{A}} = {\\mathbf{U}}{\\mathbf{\\Sigma}}{\\mathbf{V}}^T$. The computation of the SVD consists in finding the eigenvalues and eigenvectors of $\\mathbf{A}\\mathbf{A}^T$ and $\\mathbf{A}^T\\mathbf{A}$. The eigenvectors of $\\mathbf{A}^T\\mathbf{A}$ form the columns of $\\mathbf{V}$, while the eigenvectors of $\\mathbf{A}\\mathbf{A}^T$ form the columns of $\\mathbf{U}$. As stated earlier, the singular values in $\\mathbf{\\Sigma}$ are the square roots of the eigenvalues of either $\\mathbf{A}\\mathbf{A}^T$ or $\\mathbf{A}^T\\mathbf{A}$.\n",
    "\n",
    "The singular values are the diagonal entries of the matrix $\\mathbf{\\Sigma}$ and are sorted in decreasing order, and the eigenvectors in U and V matrices are reordered accordingly. If $v$ is an eigenvector, then $-v$ is also an eigenvector, and different decompositions can lead to different eigenvectors signs, so we will verify the following equation to ensure that the signs are the same:\n",
    "$${\\mathbf{A}}\\mathbf{V} = \\mathbf{U}\\mathbf{\\Sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1cd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "A = np.random.random((n, n))\n",
    "\n",
    "def svd(A):\n",
    "    S, U = la.eigh(A.dot(A.T))\n",
    "    S, V = la.eigh(A.T.dot(A))\n",
    "    S = np.sqrt(S)\n",
    "    idx = S.argsort()[::-1]\n",
    "    S = S[idx]\n",
    "    U = U[:,idx]\n",
    "    V = V[:,idx]\n",
    "    # matrix of 1's and -1's to satisfy the equality AV = US\n",
    "    MS = 2 * np.isclose(A.dot(V), (U.dot(np.diag(S)))) - 1\n",
    "    V = MS * V\n",
    "    return U, S, V.T\n",
    "\n",
    "\n",
    "# custom function call\n",
    "U, S, V = svd(A)\n",
    "S = np.diag(S) # vector to matrix\n",
    "\n",
    "# SVD using numpy's built-in function\n",
    "Unp, Snp, Vnp = la.svd(A)\n",
    "Snp = np.diag(Snp)\n",
    "\n",
    "print(np.allclose(A, U @ S @ V))\n",
    "print(np.allclose(A, Unp @ Snp @ Vnp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3719305",
   "metadata": {},
   "source": [
    "## 2. Linear Regression\n",
    "\n",
    "### 2.1 Theory\n",
    "\n",
    "Linear regression aims to find the parameter vector $ \\Theta \\in \\mathbb{R}^d $ that best fits a set of observations $ (X, y) $, where $X \\in \\mathbb{R}^{n \\times d} $ is the design matrix and $ y \\in \\mathbb{R}^n $ is the vector of target values. The model assumes a linear relationship given by the matrix equation:\n",
    "$$\n",
    "X \\Theta = y.\n",
    "$$\n",
    "When $ X $ does not have full column rank or when the system is overdetermined (i.e., $ n > d $), the equation may not have an exact solution. In such cases, we seek the least-squares solution that minimizes the squared error $ \\| X \\Theta - y \\|_2^2 $. The analytical solution is given by:\n",
    "$$\n",
    "\\Theta = X^{+} y,\n",
    "$$\n",
    "where $ X^{+} $ denotes the Moore–Penrose pseudoinverse of $ X $. To compute $ X^{+} $, we can use the singular value decomposition (SVD). Let $ X = U \\Sigma V^\\top $ be the SVD of $ X $, then the pseudoinverse is given by $X^{+} = V \\Sigma^{+} U^\\top$, where $ \\Sigma^{+} $ is formed by taking the inverse of each non-zero singular value in $ \\Sigma $ (in practice, each singular value superior to a small threshold) and padding with zeros if the singular values are inferior to the threshold. This approach ensures a stable and general solution, even when $ X $ is not of full rank.\n",
    "\n",
    "Note: The solution can also be given by the normal equation, but is less stable and more costly to compute due to the matrix inversion: $ \\Theta = (X^{T}X)^{-1}X^{T} y $\n",
    "\n",
    "### 2.2 Example with two parameters\n",
    "\n",
    "In simple linear regression with two parameters, we aim to model the relationship between a scalar input $x$ and a target variable $y$ as a linear function:\n",
    "$$\n",
    "y = \\theta_0 + \\theta_1 x,\n",
    "$$\n",
    "where $\\theta_0$ is the intercept and $\\theta_1$ is the slope. Given a dataset of $n$ observations $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$, we can write the model in matrix form as:\n",
    "$$\n",
    "X \\Theta = y,\n",
    "$$\n",
    "with:\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_n \\\\\n",
    "\\end{bmatrix}, \\quad\n",
    "\\Theta = \\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\end{bmatrix}, \\quad\n",
    "y = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "To find the best linear fit in the least-squares sense, we seek the parameter vector $\\Theta$ given by:\n",
    "$$\n",
    "\\Theta = X^+ y \\qquad with \\qquad X^+ = V \\Sigma^+ U^\\top\n",
    "$$\n",
    "\n",
    "This leads to a stable and general method for solving the linear regression problem, even when $X$ is not of full rank. For this simple 2-parameter regression, the formulas can be computed analytically:\n",
    "\n",
    "$$\n",
    "\\theta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}   \n",
    "$$\n",
    "$$\n",
    "\\theta_0 = \\bar{y} - \\theta_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ represent the mean values of those vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs X\n",
    "nb_points = 100\n",
    "X = 2 * np.random.rand(nb_points, 1)\n",
    "X_b = add_dummy_feature(X) # add a column of 1s\n",
    "\n",
    "# Outputs y\n",
    "bias = 4\n",
    "slope = 3\n",
    "y = bias + slope * X + np.random.rand(nb_points, 1) # add noise\n",
    "\n",
    "\n",
    "\n",
    "# SVD\n",
    "U, S, V = la.svd(X_b)\n",
    "St = np.diag([1/s if s > 1e-5 else 0 for s in S]) # inverse singular values\n",
    "St = np.hstack([St, np.zeros((2, nb_points-2))]) # transform a (2*2) matrix to a (100*2) matrix\n",
    "theta_svd = V.T @ St @ U.T @ y\n",
    "\n",
    "# PSEUDO INVERSE\n",
    "theta_la = np.linalg.pinv(X_b) @ y\n",
    "\n",
    "# NORMAL EQUATION\n",
    "theta_normal = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "\n",
    "# LINEAR REGRESSION\n",
    "model = LinearRegression()\n",
    "model.fit(X, y.ravel())\n",
    "theta_lr = np.array([[model.intercept_], model.coef_])\n",
    "\n",
    "# ANALYTICAL FORMULAS\n",
    "x_mean = np.mean(X)\n",
    "y_mean = np.mean(y)\n",
    "theta_1 = np.sum((X - x_mean) * (y - y_mean)) / np.sum((X - x_mean) ** 2)\n",
    "theta_0 = y_mean - theta_1 * x_mean\n",
    "theta_formula = [[theta_0], [theta_1]]\n",
    "\n",
    "\n",
    "\n",
    "# CONFIRMATION\n",
    "print(np.allclose(theta_svd, theta_la))\n",
    "print(np.allclose(theta_svd, theta_lr))\n",
    "print(np.allclose(theta_svd, theta_normal))\n",
    "print(np.allclose(theta_svd, theta_formula))\n",
    "\n",
    "\n",
    "\n",
    "x_line = np.array([[0], [2]]) # form a line from 2 points\n",
    "y_line = add_dummy_feature(x_line) @ theta_svd # 1 * theta0 + x * theta1\n",
    "\n",
    "\n",
    "# Plot graph\n",
    "plt.plot(X, y, \"b.\", label=\"data points\") \n",
    "plt.plot(x_line, y_line, \"red\", label=\"fitted line\")\n",
    "plt.xlabel(\"Input x\")\n",
    "plt.ylabel(\"Output y\")\n",
    "plt.title(\"Regression line computed with SVD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff5394",
   "metadata": {},
   "source": [
    "### 2.3 Time complexity\n",
    "\n",
    "Let's see which algorithm is faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time lin reg  : 0.633 ms, variance : 3.77 ns\n",
      "Execution time svd      : 0.380 ms, variance : 9.76 ns\n",
      "Execution time pinverse : 0.233 ms, variance : 0.64 ns\n",
      "Execution time normal   : 0.162 ms, variance : 0.03 ns\n"
     ]
    }
   ],
   "source": [
    "n_points = 100 \n",
    "n_features = 10 \n",
    "X = 2 * np.random.rand(n_points, n_features)\n",
    "y = 4 + 3 * X + np.random.rand(n_points, n_features)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "def svd(X, y):\n",
    "    X_b = add_dummy_feature(X)\n",
    "    U, S, V = la.svd(X_b)\n",
    "    St = np.diag([1/s if s > 1e-5 else 0 for s in S])\n",
    "    St = np.hstack([St, np.zeros((n_features+1, n_points-n_features-1))])\n",
    "    theta_svd = V @ St @ U.T @ y\n",
    "    return theta_svd\n",
    "\n",
    "def pinverse(X, y):\n",
    "    X_b = add_dummy_feature(X)\n",
    "    theta_la = np.linalg.pinv(X_b) @ y\n",
    "    return theta_la\n",
    "\n",
    "def normal(X, y):\n",
    "    X_b = add_dummy_feature(X)\n",
    "    theta_normal = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "    return theta_normal\n",
    "\n",
    "def lr(X, y):\n",
    "    model.fit(X, y)\n",
    "    theta_lr = np.vstack((model.intercept_, model.coef_))\n",
    "    return theta_lr\n",
    "\n",
    "\n",
    "exe_time_svd = np.array(timeit.repeat('svd(X, y)', globals=globals(), number=100)) / 1e2\n",
    "exe_time_pin = np.array(timeit.repeat('pinverse(X, y)', globals=globals(), number=100)) / 1e2\n",
    "exe_time_nor = np.array(timeit.repeat('normal(X, y)', globals=globals(), number=100)) / 1e2\n",
    "exe_time_lin = np.array(timeit.repeat('lr(X, y)', globals=globals(), number=100)) / 1e2\n",
    "\n",
    "strings = ['lin reg  ', 'svd      ', 'pinverse ', 'normal   ']\n",
    "times = [exe_time_lin, exe_time_svd, exe_time_pin, exe_time_nor]\n",
    "\n",
    "for str, tim in zip(strings, times):\n",
    "    print(f\"Execution time {str}: {(np.mean(tim)/1e-3):.3f} ms, variance : {(np.var(tim)/1e-9):.2f} ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a08c044",
   "metadata": {},
   "source": [
    "Note: even if the normal equation gives faster time execution, this algorithm is not commonly used as it is unstable, especially if the matrix is poorly conditionned or singular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee145d",
   "metadata": {},
   "source": [
    "## 3. SVD applied to image compression\n",
    "\n",
    "### 3.1 Approximation error\n",
    "\n",
    "How well can we approximate a matrix $\\mathbf{A}$ with rank-1 matrix? Mathematics tells us that ${\\mathbf{A}}_k$ is the best rank-$k$ approximation of ${\\mathbf{A}}$ in the sense of the Euclidean norm, and the singular value $\\sigma_{k+1}$ represents the lower bound of the approximation error:\n",
    "$$\n",
    "\\min_{{\\operatorname{rg}({\\mathbf{B}})}=k}\\|{\\mathbf{A}}-{\\mathbf{B}}\\|_2 = \\|{\\mathbf{A}}-{\\mathbf{A}}_k\\|_2 = \\sigma_{k+1}\n",
    "$$\n",
    "$$\n",
    "\\text{ where } {\\mathbf{A}}_k = \\sum_{i=1}^k{\\mathbf{A}}_i = \\sum_{i=1}^k\\sigma_i{\\mathbf{u}}_i {\\mathbf{v}}_i^T\n",
    "$$\n",
    "\n",
    "Let's see how the number of singular values kept ($k$) affects the reconstruction error of the matrix $\\mathbf{A}_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "index = np.arange(1, n+1)\n",
    "A = np.random.random((n, n))\n",
    "U, S, V = la.svd(A)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title(\"Singular value $\\sigma_k$\")\n",
    "plt.bar(index, S)\n",
    "plt.xticks(index)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Ak matrix contruction \n",
    "norms = []\n",
    "for i in range(n):\n",
    "    Ak = U[:, :i] @ np.diag(S[:i]) @ V[:i, :]\n",
    "    norms.append(la.norm((A - Ak), 2))\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title(\"${||{\\mathbf{A}}-{\\mathbf{A}_k}||}_2$\")\n",
    "plt.bar(index, norms)\n",
    "plt.xticks(index)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dbbe5c",
   "metadata": {},
   "source": [
    "### 3.2 Image compression \n",
    "\n",
    "This equality also allows us to compress information contained in a matrix with a minimumized lost information. Let's see the visual impact of this decomposition on a 512*512 black and white image (each pixel is stocked in a matrix with a value encoded on 256 grey levels). We are going to reconstruct this image with 1, 5, 10, 15, 20, 50, 100, 200 singular values and observe how it affect it. But first, let's see what a single feature ${\\mathbf{A}}_i$ looks like, defined by:\n",
    "$$\n",
    "{\\mathbf{A}}_i=\\sigma_i{\\mathbf{u}}_i {\\mathbf{v}}_i^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "image = data.astronaut()\n",
    "image = np.asarray(rgb2gray(image))\n",
    "\n",
    "\n",
    "# SVD\n",
    "U,S,V = la.svd(image)\n",
    "\n",
    "# Singular values number\n",
    "comps = [1, 5, 10, 15, 20, 50, 100, 200, 400]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i in range(len(comps)):\n",
    "    Ai = S[comps[i]] * np.outer(U[:, comps[i]], V[comps[i], :])\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(Ai, cmap='gray')\n",
    "    plt.title(f'Single feature no {comps[i]}.')\n",
    "    frame1 = plt.gca()\n",
    "    frame1.axes.get_xaxis().set_ticks([])\n",
    "    frame1.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i in range(len(comps)):\n",
    "    Ak = U[:, :comps[i]] @ np.diag(S[:comps[i]]) @ V[:comps[i], :]\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(Ak, cmap='gray')\n",
    "    plt.title(f\"Sum of the first {comps[i]} features\")\n",
    "    frame1 = plt.gca()\n",
    "    frame1.axes.get_xaxis().set_ticks([])\n",
    "    frame1.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Original image (with 512 features)')\n",
    "frame1 = plt.gca()\n",
    "frame1.axes.get_xaxis().set_ticks([])\n",
    "frame1.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "n = 200\n",
    "Ak = U[:, :n] @ np.diag(S[:n]) @ V[:n, :]\n",
    "plt.imshow(Ak, cmap='gray')\n",
    "plt.title(f'Image with {n} features')\n",
    "frame1 = plt.gca()\n",
    "frame1.axes.get_xaxis().set_ticks([])\n",
    "frame1.axes.get_yaxis().set_ticks([])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d31cdf",
   "metadata": {},
   "source": [
    "The error of the reconstructed matrices decrease exponentially with the number of singular values used (it follows the previous histogram of $\\|{\\mathbf{A}}-{\\mathbf{A}_k}\\|_2$ in Part 3.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a950269",
   "metadata": {},
   "source": [
    "### 3.3 Function approximation\n",
    "\n",
    "In the same way, we can reconstruct functions with few SVD features. Regular functions are smooth, so very few features will be necessary. This compression can be usefull when stocking functions with many data points or high dimensionnal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bird(x,y):\n",
    "    return np.sin(x)*(np.exp(1-np.cos(y))**2) + np.cos(y)*(np.exp(1-np.sin(x))**2) + (x-y)**2\n",
    "\n",
    "x = np.linspace(-2*math.pi, 2*math.pi,100)\n",
    "y = np.linspace(-2*math.pi, 2*math.pi,100)\n",
    "X, Y = np.meshgrid(x,y)\n",
    "Z = bird(X,Y)\n",
    "birdarray = np.asarray(Z)\n",
    "\n",
    "# SVD\n",
    "U,S,V = la.svd(birdarray)\n",
    "\n",
    "\n",
    "# Singular values number\n",
    "comps = [1, 2, 3, 4, 5, 10, 20, 30, 50]\n",
    "\n",
    "fig=plt.figure(figsize=(12, 10))\n",
    "for i in range(len(comps)):\n",
    "    bird_rank = S[comps[i]] * np.outer(U[:, comps[i]], V[comps[i], :])\n",
    "    ax=fig.add_subplot(330+i+1,projection='3d')\n",
    "    ax.plot_surface(X,Y,bird_rank,cmap='coolwarm')\n",
    "    plt.title(f'Single feature no {comps[i]}')\n",
    "    frame1 = plt.gca()\n",
    "    frame1.axes.get_xaxis().set_ticks([])\n",
    "    frame1.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "fig=plt.figure(figsize=(12, 10))\n",
    "for i in range(len(comps)):\n",
    "    bird_rank = U[:, :comps[i]] @ np.diag(S[:comps[i]]) @ V[:comps[i], :]\n",
    "    ax=fig.add_subplot(330+i+1,projection='3d')\n",
    "    ax.plot_surface(X,Y,bird_rank,cmap='coolwarm')\n",
    "    plt.title(f'Sum of the first {comps[i]} features')\n",
    "    frame1 = plt.gca()\n",
    "    frame1.axes.get_xaxis().set_ticks([])\n",
    "    frame1.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "fig=plt.figure(figsize=(12, 6))\n",
    "ax=fig.add_subplot(121,projection='3d')\n",
    "ax.plot_surface(X,Y,Z,cmap='coolwarm')\n",
    "plt.title('Original function (100 features)')\n",
    "ax=fig.add_subplot(122,projection='3d')\n",
    "bird_rank = U[:, :5] @ np.diag(S[:5]) @ V[:5, :]\n",
    "ax.plot_surface(X,Y,bird_rank,cmap='coolwarm')\n",
    "plt.title('Fonction with 5 features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba48f0",
   "metadata": {},
   "source": [
    "## 4. PCA\n",
    "\n",
    "Principal components analysis is the main method used for dimension reduction.\n",
    "It performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. The variance of a dataset projected on an eigenvector is proportionnal to the eigenvalue of this eigenvector, so if we select the eigenvectors with higher eigenvalues, we can construct the hyperplane where the projected dataset will have the maximum variance. With maximum variance, maximum information is preserved during the projection. \n",
    "$$\n",
    "\\boxed{variance \\propto eigenvalues}\n",
    "$$\n",
    "\n",
    "\n",
    "### 4.1 PCA Implementations\n",
    "\n",
    "Let $ X \\in \\mathbb{R}^{n \\times d} $ be a data matrix with $ n = 100 $ samples and $ d = 10 $ features. The goal of Principal Component Analysis (PCA) is to reduce the dimensionality of $ X $ by projecting it onto a new set of orthogonal axes (principal components) that capture the maximum variance in the data.\n",
    "\n",
    "We begin by centering the data:\n",
    "\n",
    "$$\n",
    "X_{\\text{c}} = X - \\mu, \\quad \\text{where } \\mu = \\frac{1}{n} \\sum_{i=1}^n X_i\n",
    "$$\n",
    "\n",
    "#### 1. PCA via Covariance Matrix (Eigen-Decomposition)\n",
    "\n",
    "The eigenvectors and eigenvalues of a covariance (or correlation) matrix are the core of PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\n",
    "\n",
    "The classic approach to PCA is to perform the eigendecomposition on the covariance matrix, which is a matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:\n",
    "$$\n",
    "Cov(X, Y) = \\frac{\\sum(x_i - \\bar{x}) (y_i - \\bar{y})}{N-1}\n",
    "$$\n",
    "\n",
    "To compute the complete covariance matrix, we use the following formula:\n",
    "\n",
    "$$\n",
    "M_{cov} = \\frac{1}{n-1} X_{\\text{c}}^\\top X_{\\text{c}} \\in \\mathbb{R}^{d \\times d}\n",
    "$$\n",
    "\n",
    "We then perform an eigen-decomposition of $ M_{cov} $:\n",
    "\n",
    "$$\n",
    "M_{cov} = V \\Lambda V^\\top\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ V \\in \\mathbb{R}^{d \\times d} $ contains the eigenvectors,\n",
    "- $ \\Lambda \\in \\mathbb{R}^{d \\times d} $ is a diagonal matrix of eigenvalues.\n",
    "\n",
    "We sort the eigenvalues in descending order and select the top $ k = 2 $ associated eigenvectors $ V_k \\in \\mathbb{R}^{d \\times k} $. The data is projected onto these components:\n",
    "\n",
    "$$\n",
    "\\boxed{X_{\\text{PCA}} = X_{\\text{c}} \\cdot V_k}\n",
    "$$\n",
    "\n",
    "#### 2. PCA via Singular Value Decomposition (SVD)\n",
    "\n",
    "Instead of computing the covariance matrix, we can directly apply the Singular Value Decomposition (SVD) to the data:\n",
    "\n",
    "$$\n",
    "X_{\\text{c}} = U \\Sigma V^\\top\n",
    "$$\n",
    "\n",
    "As the matrices $U$ and $V$ are orthogonal ($U\\cdot U^T = Id$), it can be shown that:\n",
    "\n",
    "$$\n",
    "X_{\\text{c}}^\\top X_{\\text{c}} = V \\Sigma^2 V^\\top\n",
    "$$\n",
    "\n",
    "So the right singular vectors $ V $ are the same as the eigenvectors of the covariance matrix, and the singular values squared are proportional to the eigenvalues: $\\lambda_i = \\frac{\\sigma_i^2}{n-1}$. The Principal Components are the eigenvectors of $X$, given by the columns of $ V $. To project the data onto the first $ k $ principal components (here, $ k = 2 $), we compute:\n",
    "$$\n",
    "\\boxed{X_{\\text{PCA}} = X_{\\text{c}} \\cdot V_k}\n",
    "$$\n",
    "\n",
    "Advantage: This approach avoids explicitly computing the covariance matrix, making it more stable and computationally efficient, especially for large or high-dimensional data.\n",
    "\n",
    "\n",
    "#### 3. PCA via Scikit-learn\n",
    "\n",
    "The scikit-learn implementation automates the steps above. Internally, it also uses SVD for performance:\n",
    "\n",
    "- It centers the data automatically.\n",
    "- It computes the top $ k $ components using a truncated SVD.\n",
    "\n",
    "Since PCA components are defined up to a sign (if $v$ is an eigenvector, then $-v$ is also an eigenvector), the projections might differ by their sign but are otherwise equivalent. We will check the equality between the absolute values of the projections. The results are numerically equivalent, confirming the theoretical result: the eigen-decomposition of the covariance matrix is equivalent to the SVD of the centered data.  \n",
    "\n",
    "In the next code cell, we illustrate how to perform Principal Component Analysis (PCA) using the manual Eigenvectors decomposition and the Singular Value Decomposition, and compare the results with the PCA implementation provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b15ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "# Data X of dimension (n_samples, n_features)\n",
    "X = np.random.normal(size=(100, 10))\n",
    "Xc = X - np.mean(X, axis=0)\n",
    "\n",
    "\n",
    "## 1. PCA with covariance matrix\n",
    "# Correlation matrix\n",
    "cov_matrix = np.cov(Xc.T)\n",
    "# Decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "# Order with decreasing eigenvalues \n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "# Projection (n_features, 2)\n",
    "X_pca_cov = Xc @ eigenvectors[:, :2]\n",
    "\n",
    "\n",
    "## 2. PCA with SVD\n",
    "U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "# Selection of the 2 first eigenvectors (principal components) contained in V \n",
    "Vk = Vt[:2].T  # Projection matrix (features x 2)\n",
    "X_pca_svd = Xc @ Vk\n",
    "\n",
    "\n",
    "## 3. PCA with scikit-learn\n",
    "pca = PCA(n_components=2)\n",
    "X_pca_sklearn = pca.fit_transform(X) # centers data by default\n",
    "\n",
    "\n",
    "\n",
    "# Results comparison\n",
    "# PCA decomposition are unique modulo their sign, so we compare absolute values\n",
    "print(np.allclose(np.abs(X_pca_sklearn), np.abs(X_pca_cov)))\n",
    "print(np.allclose(np.abs(X_pca_sklearn), np.abs(X_pca_svd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b29bad",
   "metadata": {},
   "source": [
    "### 4.2 Visual Interpretation\n",
    "\n",
    "In the next cell, we construct a synthetic 2D dataset to illustrate the principles and geometric intuition behind PCA. We begin by generating normally distributed points and stretching them along one axis to form an ellipsoidal shape, then apply a rotation to introduce correlation between the variables. PCA is then applied to this rotated dataset to identify the directions of maximum variance (its principal components). These components are visualized as red arrows overlaying the data, and we compare the original and transformed data through histograms of their projections onto both the original axes and the principal axes. This example shows how PCA decorrelates data and concentrates variance along orthogonal directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ellipsoidal 2D dataset\n",
    "X = np.random.normal(size=(300, 2))\n",
    "stretch_factors = np.array([5, 1])\n",
    "X_stretched = X * stretch_factors\n",
    "\n",
    "# Rotation \n",
    "theta = np.radians(-40)\n",
    "R_x = np.array([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta),  np.cos(theta)]\n",
    "])\n",
    "X_rotated = X_stretched @ R_x \n",
    "\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_rotated)\n",
    "components = pca.components_                  # Eigenvectors\n",
    "explained_variance = pca.explained_variance_  # Eigenvalues\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_rotated[:, 0], X_rotated[:, 1], alpha=0.6)\n",
    "\n",
    "for i, (length, vector) in enumerate(zip(explained_variance, components)):\n",
    "    vector_scaled = np.sqrt(length) * vector * 2\n",
    "    plt.arrow(0, 0, vector_scaled[0], vector_scaled[1], color='red', width=0.15, head_width=0.6)\n",
    "    plt.arrow(0, 0, -i+1, i, color='lightgreen', width=0.15, head_width=0.6)\n",
    "plt.title(\"Principal Components of the Dataset\")\n",
    "plt.axis('equal')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Common scale limit\n",
    "x_min, x_max = -20, 20\n",
    "axis = [\"x\", \"y\"]\n",
    "\n",
    "# Distributions plotting\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "for i in range(2): \n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.hist(X_rotated[:, i], bins=40-i*10, color=f\"C{i}\", alpha=0.7)\n",
    "    plt.title(f\"Projection on {axis[i]} axis\")\n",
    "    plt.xlabel(f\"Axis {i+1}\\nVariance: {np.var(X_rotated[:, i]):.1f}\")\n",
    "    plt.ylabel(\"Number of values\")\n",
    "    plt.xlim(x_min, x_max)\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 4, i+3)\n",
    "    plt.hist(X_pca[:, i], bins=40-i*30, color=f\"C{i+2}\", alpha=0.7)\n",
    "    plt.title(f\"Projection on PC {i+1} axis\")\n",
    "    plt.xlabel(f\"PC {i+1}\\nVariance: {np.var(X_pca[:, i]):.1f}\")\n",
    "    plt.ylabel(\"Number of values\")\n",
    "    plt.xlim(x_min, x_max)\n",
    "\n",
    "plt.suptitle(\"Points distributions on the Principal Components axis\", fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"Explained variance ratio of PC1 and PC2: {pca.explained_variance_ratio_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0fdea",
   "metadata": {},
   "source": [
    "### 4.3 Clustering \n",
    "\n",
    "Dimensionnality reduction algorithms allows us to visualize complex dataset. After a PCA, we can plot on the 2 first Principal Components the projected dataset and see the formed clusters. Let's try this on the simple iris dataset containing 4 features of different flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "flower_names = data.target_names\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, flower in enumerate(flower_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=flower)\n",
    "plt.title(\"PCA on the Iris dataset\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f7659",
   "metadata": {},
   "source": [
    "We can test the PCA algorithm on a much more complex dataset: MNIST. It contains hand-written digits drawn on a 28*28 square in grayscale, so the dataset has 784 features (variables) per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser=\"auto\")\n",
    "X, y = mnist.data[:3000], mnist.target[:3000]\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca_reduced = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(X_pca_reduced[:, 0], X_pca_reduced[:, 1], c=y.astype(np.int8), cmap=\"jet\", alpha=0.8)\n",
    "plt.title(\"PCA on the MNIST dataset\")\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2e33d",
   "metadata": {},
   "source": [
    "PCA is a linear projection method, it is not ideal for complex and non linear data. MNIST is composed of hand-written numbers, and their variations (style, orientation, pen used) doesn't match with a simple linear projection. That's why we obtain a blured cloud without well separated clusters. Moreover, the PCA algorithm tries to maximize the variance between variables, which doesn't lead to distinct clusters.  \n",
    "\n",
    "Let's try another dimentionnality reduction algorithm: t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\", random_state=42)\n",
    "X_reduced = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(13, 10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y.astype(np.int8), cmap=\"jet\", alpha=0.8)\n",
    "plt.title(\"T-SNE on the MNIST dataset\")\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa473f0",
   "metadata": {},
   "source": [
    "Impressive! Most digits are nicely separated from the others, even though t-SNE wasn't given the targets: it just identified clusters of similar images. There is still a bit of overlap. For example, the 3s and the 5s overlap a lot (on the right side of the plot), and so do the 4s and the 9s (in the top-right corner). It makes sens as these digits are similar when hand-written."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
