{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aabc147",
   "metadata": {},
   "source": [
    "# Scaling Laws in Machine Learning\n",
    "\n",
    "Ever found yourself asking questions like:\n",
    "\n",
    "* \"If I collect twice as much data, how much better will my model *actually* get?\"\n",
    "* \"What is the performance boost of doubling my model's parameters number?\"\n",
    "* \"How can I predict the performance of a model without having to run every single experiment?\"\n",
    "\n",
    "If so, you're in the right place! These are super common (and important!) questions in the world of machine learning. The exciting part is that the way a model's performance improves is determined by maths rules, and these rules are what we call **scaling laws**. They describe how a model's error or performance changes as we scale up certain resources, like the amount of training data or the size of the model itself. \n",
    "\n",
    "**What's Inside This Notebook?**\n",
    "\n",
    "In this notebook, we'll take a look at the mathematical intuition that underpins these scaling laws, focusing on two areas:\n",
    "\n",
    "1.  **Scaling with Dataset Size:** We'll first investigate how a model's error typically decreases as we feed it more training data.\n",
    "2.  **Scaling with Model Parameters:** Next, we'll focus on how does increasing the complexity or the number of parameters in a model affect its error? \n",
    "3.  **Practical experiment:** Finally, we will code these two scaling laws with a regression model (RandomForest) and verify these theoretical results.\n",
    "\n",
    "Ready to dive in? Let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d83858",
   "metadata": {},
   "source": [
    "## 1. Scaling law with respect to dataset size\n",
    "\n",
    "### 1.1 The Basic Idea: Learning from Limited Data\n",
    "\n",
    "When we train a machine learning model, we're trying to find a model that works well not just on the training data we have, but also on new, unseen data. The error on unseen data is the **true error**, let's call it $R(h)$ for a model $h$. The error on our training data (of size $N$) is the **empirical error**, $\\hat{R}_N(h)$.\n",
    "\n",
    "We hope that if $\\hat{R}_N(h)$ is small, then $R(h)$ will also be small. However, because our training data is just a sample, $\\hat{R}_N(h)$ is only an estimate of $R(h)$. The key question is: how good is this estimate?\n",
    "\n",
    "### 1.2 Deriving the scaling law\n",
    "\n",
    "Imagine, for a moment, that we have just one fixed model $h$ (we didn't even train it, we just picked it). We want to know its true error $R(h)$. We can estimate this by calculating its error $\\hat{R}_N(h)$ on $N$ data points.\n",
    "\n",
    "Each data point gives us a piece of information about whether $h$ is correct or not. Let $Z_i = 1$ if $h$ is wrong on the $i$-th data point, and $Z_i = 0$ if it's right.\n",
    "The true error is the average of $Z_i$ over all possible data:\n",
    "$$ R(h) = \\mathbb{E}[Z_i] $$\n",
    "The empirical error is the average over our sample:\n",
    "$$ \\hat{R}_N(h) = \\frac{1}{N} \\sum_{i=1}^{N} Z_i $$\n",
    "\n",
    "Statistical theory (specifically, Hoeffding's inequality, which is a concentration inequality) tells us how close the sample average $\\hat{R}_N(h)$ is likely to be to the true average $R(h)$. It states that the probability of the difference being large is small:\n",
    "$$ P\\left( |R(h) - \\hat{R}_N(h)| \\ge \\epsilon \\right) \\le 2 e^{-2N\\epsilon^2} $$\n",
    "This formula means:\n",
    "- The probability that the true error $R(h)$ and the empirical error $\\hat{R}_N(h)$ differ by more than some amount $\\epsilon$ decreases very quickly as $N$ (the dataset size) increases.\n",
    "- It also decreases as $\\epsilon$ (the allowed difference) increases.\n",
    "\n",
    "\n",
    "Now, let's flip this around. Suppose we want to be reasonably confident (say, with probability $1-\\delta$, where $\\delta$ is small, like 0.05) about how much $R(h)$ can differ from $\\hat{R}_N(h)$. We can say that, with high probability:\n",
    "$$ |R(h) - \\hat{R}_N(h)| \\le \\sqrt{\\frac{\\ln(2/\\delta)}{2N}} $$\n",
    "This simplifies to:\n",
    "$$ MSE(h) \\approx O\\left(\\frac{1}{\\sqrt{N}}\\right) $$\n",
    "So, for a single, fixed model, the \"uncertainty\" in our estimate of the true error decreases proportionally to $1/\\sqrt{N}$.\n",
    "\n",
    "\n",
    "### 1.3 Conclusion\n",
    "\n",
    "In practice, Machine Learning models are never perfect and there always exist a bias. We can model it with $E_{\\text{bias}}$ which represent some underlying bias – the error that even an infinitely large dataset couldn't eliminate and C a constant.\n",
    "$$ MSE(h) = \\frac{C}{\\sqrt{N}} + E_{\\text{bias}}$$\n",
    "$$ MSE(h) = A \\cdot N^{-\\alpha} + B \\quad \\Rightarrow \\quad \\boxed{\\alpha = 1/2}$$\n",
    "\n",
    "This shows a common way the $1/\\sqrt{N}$ scaling arises: it's often linked to how quickly our statistical estimation error (the uncertainty from using a finite sample) decreases as we get more data. While more complex scaling laws (like $N^{-\\alpha}$ with different $\\alpha$) exist, this $1/\\sqrt{N}$ behavior is a fundamental reference point from statistical learning theory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907791ef",
   "metadata": {},
   "source": [
    "## 2. Scaling law with respect to parameters number\n",
    "\n",
    "### 2.1 Problem Formulation\n",
    "\n",
    "We train a random forest with $N$ independent regression trees. Each tree is an estimator of the target function. We assume:\n",
    "\n",
    "- Each tree has a prediction variance of $\\sigma^2$\n",
    "- Trees are independent (or weakly correlated)\n",
    "- The final prediction is the average of the individual tree predictions\n",
    "\n",
    "\n",
    "### 2.2 Variance of the Averaged Model\n",
    "\n",
    "Let $Y_i$ be the prediction of the $i$-th tree. The ensemble prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\frac{1}{N} \\sum_{i=1}^N Y_i\n",
    "$$\n",
    "\n",
    "If the trees are independent with variance $\\operatorname{Var}(Y_i) = \\sigma^2$, then the variance of the average is:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{Y}) = \\frac{\\sigma^2}{N}\n",
    "$$\n",
    "\n",
    "This is a classical property of the mean of independent estimators.\n",
    "\n",
    "### 2.3 Mean Squared Error (MSE)\n",
    "\n",
    "The total error can be written using the bias-variance decomposition:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\underbrace{\\text{Bias}^2}_{\\text{model bias}} + \\underbrace{\\text{Variance}}_{\\frac{\\sigma^2}{N}}\n",
    "$$\n",
    "\n",
    "In a random forest, the trees are typically deep and hence low-bias (bias $\\approx 0$)\n",
    "\n",
    "So we can approximate:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(N) \\approx \\frac{A}{N} + B\n",
    "$$\n",
    "\n",
    "Which is of the form:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(N) = A \\cdot N^{-\\alpha} + B \\quad \\Rightarrow \\quad \\boxed{\\alpha = 1}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a9243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:02<00:25,  2.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params, size \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(estimators_nb, train_sizes), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_sizes)):\n\u001b[1;32m     45\u001b[0m     \n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Parameters number changes\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     50\u001b[0m     mse_params \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "### Load dataset ###\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Data nomalization\n",
    "scaler_X = StandardScaler()\n",
    "X = scaler_X.fit_transform(X)\n",
    "\n",
    "# Train - test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Training models ###\n",
    "\n",
    "n_points = 10\n",
    "\n",
    "# Dataset sizes to test (full dataset fractions)\n",
    "fractions = np.linspace(1e-2, 1, n_points) # 1% to 100%\n",
    "train_sizes = [int(f * len(X_train_full)) for f in fractions]\n",
    "\n",
    "# Number of parameters to test\n",
    "estimators_nb = np.linspace(1, 200, n_points, dtype=int)\n",
    "\n",
    "\n",
    "errors_samples = []\n",
    "errors_params = []\n",
    "\n",
    "for params, size in tqdm(zip(estimators_nb, train_sizes), total=len(train_sizes)):\n",
    "    \n",
    "    # Parameters number changes\n",
    "    model = RandomForestRegressor(n_estimators=params)\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse_params = mean_squared_error(y_test, y_pred)\n",
    "    errors_params.append(mse_params)\n",
    "    \n",
    "    # Dataset size changes\n",
    "    X_train = X_train_full[:size]\n",
    "    y_train = y_train_full[:size]\n",
    "    model = RandomForestRegressor(n_estimators=100) # Default\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse_samples = mean_squared_error(y_test, y_pred)\n",
    "    errors_samples.append(mse_samples)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### Compute the regression for the scaling laws ###\n",
    "\n",
    "def scaling_law(N, A, alpha, B):\n",
    "    return A * N**(-alpha) + B\n",
    "\n",
    "def regression(x, y):\n",
    "    \"\"\" Compute the regression of the points (x, y) on the scaling law function. \"\"\"\n",
    "    \n",
    "    # Fit of A, alpha, B\n",
    "    params, covariance = curve_fit(scaling_law, x, y, bounds=([0, 0, 0], [np.inf, 1, np.inf]))\n",
    "    A, alpha, B = params\n",
    "\n",
    "    print(f\"A = {A:.3f}, alpha = {alpha:.3f}, B = {B:.3f}\")\n",
    "    print(\"Covariances :\\n\", covariance, \"\\n\")\n",
    "\n",
    "    # Lists of points\n",
    "    N_fit = np.linspace(min(x), max(x), 100)\n",
    "    Error_fit = scaling_law(N_fit, *params)\n",
    "    \n",
    "    return N_fit, Error_fit, alpha\n",
    "\n",
    "\n",
    "x_samples, y_samples, alpha_samples = regression(train_sizes, errors_samples)\n",
    "x_params, y_params, alpha_params = regression(estimators_nb, errors_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Plots the results ###\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Error vs Size\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(train_sizes, errors_samples, marker='o', label='Data')\n",
    "plt.plot(x_samples, y_samples, color='red', label=\"Fit: \" + r\"$A \\cdot N^{-\\alpha} + B$\" + f\", α={alpha_samples:.2f}\")\n",
    "plt.xlabel(\"Training dataset size\")\n",
    "plt.ylabel(\"Error (MSE)\")\n",
    "plt.title(\"Scaling Law: Error vs Dataset size\")\n",
    "plt.legend()\n",
    "\n",
    "# Error vs Parameters\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(estimators_nb, errors_params, marker='o', label='Data')\n",
    "plt.plot(x_params, y_params, color='red', label=\"Fit: \" + r\"$A \\cdot N^{-\\alpha} + B$\" + f\", α={alpha_params:.2f}\")\n",
    "plt.xlabel(\"Parameters number\")\n",
    "plt.ylabel(\"Error (MSE)\")\n",
    "plt.title(\"Scaling Law: Error vs Parameters number\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37fac7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We found the correct maths prediction for the scaling law with respect to the parameters number, but our model seems to have a smaller alpha coefficient for the dataset-size scaling law. \n",
    "In large models (deep learning), we observe in pratice a slower decay than $1/N$, often of the form $N^{-\\alpha}$ with $\\alpha \\in [0.1, 0.3]$. This slowdown can be explained by:\n",
    "* Underoptimization of the model (it does not fully converge)\n",
    "* Redundancy in the data\n",
    "* Increasing task complexity\n",
    "* The fact that the model does not have fixed capacity (the model grows with the dataset)\n",
    "\n",
    "The last point is key: **Kaplan et al.** showed that to maintain optimal training, the number of parameters $P$, the dataset size $N$, and the compute budget $C$ must grow together, following coordinated power laws. Specifically, given a 100-fold increase in computational resources (C), Kaplan et al. recommended scaling model size by approximately 28.8 times\n",
    "($P_{opt} \\propto C^{0.73}$), while increasing dataset size by only 3.5 times ($N_{opt} \\propto C^{0.27}$).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
